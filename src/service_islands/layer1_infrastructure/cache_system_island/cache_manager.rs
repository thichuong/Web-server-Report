//! Cache Manager - Unified Cache Operations
//! 
//! Manages operations across L1 (Moka) and L2 (Redis) caches with intelligent fallback.

use std::sync::Arc;
use std::time::Duration;
use std::future::Future;
use anyhow::Result;
use serde_json;
use std::sync::atomic::{AtomicU64, AtomicUsize, Ordering};
use dashmap::DashMap;
use tokio::sync::Mutex;

use super::l1_cache::L1Cache;
use super::l2_cache::L2Cache;

/// RAII cleanup guard for in-flight request tracking
/// Ensures that entries are removed from DashMap even on early return or panic
struct CleanupGuard<'a> {
    map: &'a DashMap<String, Arc<Mutex<()>>>,
    key: String,
}

impl<'a> Drop for CleanupGuard<'a> {
    fn drop(&mut self) {
        self.map.remove(&self.key);
    }
}

/// Cache strategies for different data types
#[derive(Debug, Clone)]
#[allow(dead_code)]
pub enum CacheStrategy {
    /// Real-time data - 10 seconds TTL
    RealTime,
    /// Short-term data - 5 minutes TTL  
    ShortTerm,
    /// Medium-term data - 1 hour TTL
    MediumTerm,
    /// Long-term data - 3 hours TTL
    LongTerm,
    /// Custom TTL
    Custom(Duration),
    /// Default strategy (5 minutes)
    Default,
}

impl CacheStrategy {
    /// Convert strategy to duration
    pub fn to_duration(&self) -> Duration {
        match self {
            Self::RealTime => Duration::from_secs(10),
            Self::ShortTerm => Duration::from_secs(300), // 5 minutes
            Self::MediumTerm => Duration::from_secs(3600), // 1 hour
            Self::LongTerm => Duration::from_secs(10800), // 3 hours
            Self::Custom(duration) => *duration,
            Self::Default => Duration::from_secs(300),
        }
    }
}

/// Cache Manager - Unified operations across L1 and L2
pub struct CacheManager {
    /// L1 Cache (Moka)
    l1_cache: Arc<L1Cache>,
    /// L2 Cache (Redis)
    l2_cache: Arc<L2Cache>,
    /// Statistics
    total_requests: Arc<AtomicU64>,
    l1_hits: Arc<AtomicU64>,
    l2_hits: Arc<AtomicU64>,
    misses: Arc<AtomicU64>,
    promotions: Arc<AtomicUsize>,
    /// In-flight requests to prevent Cache Stampede on L2/compute operations
    in_flight_requests: Arc<DashMap<String, Arc<Mutex<()>>>>,
}

impl CacheManager {
    /// Create new cache manager
    pub async fn new(l1_cache: Arc<L1Cache>, l2_cache: Arc<L2Cache>) -> Result<Self> {
        println!("  🎯 Initializing Cache Manager...");
        
        Ok(Self {
            l1_cache,
            l2_cache,
            total_requests: Arc::new(AtomicU64::new(0)),
            l1_hits: Arc::new(AtomicU64::new(0)),
            l2_hits: Arc::new(AtomicU64::new(0)),
            misses: Arc::new(AtomicU64::new(0)),
            promotions: Arc::new(AtomicUsize::new(0)),
            in_flight_requests: Arc::new(DashMap::new()),
        })
    }
    
    /// Get value from cache (L1 first, then L2 fallback with promotion)
    /// 
    /// This method now includes built-in Cache Stampede protection when cache misses occur.
    /// Multiple concurrent requests for the same missing key will be coalesced to prevent
    /// unnecessary duplicate work on external data sources.
    /// 
    /// # Arguments
    /// * `key` - Cache key to retrieve
    /// 
    /// # Returns
    /// * `Ok(Some(value))` - Cache hit, value found in L1 or L2
    /// * `Ok(None)` - Cache miss, value not found in either cache
    /// * `Err(error)` - Cache operation failed
    pub async fn get(&self, key: &str) -> Result<Option<serde_json::Value>> {
        self.total_requests.fetch_add(1, Ordering::Relaxed);
        
        // Fast path: Try L1 first (no locking needed)
        if let Some(value) = self.l1_cache.get(key).await {
            self.l1_hits.fetch_add(1, Ordering::Relaxed);
            return Ok(Some(value));
        }
        
        // L1 miss - implement Cache Stampede protection for L2 lookup
        let key_owned = key.to_string();
        let lock_guard = self.in_flight_requests
            .entry(key_owned.clone())
            .or_insert_with(|| Arc::new(Mutex::new(())))
            .clone();
        
        let _guard = lock_guard.lock().await;
        
        // RAII cleanup guard - ensures entry is removed even on early return or panic
        let cleanup_guard = CleanupGuard {
            map: &self.in_flight_requests,
            key: key_owned.clone(),
        };
        
        // Double-check L1 cache after acquiring lock
        // (Another concurrent request might have populated it while we were waiting)
        if let Some(value) = self.l1_cache.get(key).await {
            self.l1_hits.fetch_add(1, Ordering::Relaxed);
            // cleanup_guard will auto-remove entry on drop
            return Ok(Some(value));
        }
        
        // Check L2 cache
        if let Some(value) = self.l2_cache.get(key).await {
            self.l2_hits.fetch_add(1, Ordering::Relaxed);
            
            // Promote to L1 for faster access next time
            if let Err(_) = self.l1_cache.set_with_ttl(key, value.clone(), Duration::from_secs(300)).await {
                // L1 promotion failed, but we still have the data
                eprintln!("⚠️ Failed to promote key '{}' to L1 cache", key);
            } else {
                self.promotions.fetch_add(1, Ordering::Relaxed);
                println!("⬆️ Promoted '{}' from L2 to L1 (via get)", key);
            }
            
            // cleanup_guard will auto-remove entry on drop
            return Ok(Some(value));
        }
        
        // Both L1 and L2 miss
        self.misses.fetch_add(1, Ordering::Relaxed);
        
        // cleanup_guard will auto-remove entry on drop
        drop(cleanup_guard);
        
        Ok(None)
    }
    
    /// Get value from cache with fallback computation (enhanced backward compatibility)
    /// 
    /// This is a convenience method that combines `get()` with optional computation.
    /// If the value is not found in cache, it will execute the compute function
    /// and cache the result automatically.
    /// 
    /// # Arguments
    /// * `key` - Cache key
    /// * `compute_fn` - Optional function to compute value if not in cache
    /// * `strategy` - Cache strategy for storing computed value (default: ShortTerm)
    /// 
    /// # Returns
    /// * `Ok(Some(value))` - Value found in cache or computed successfully
    /// * `Ok(None)` - Value not in cache and no compute function provided
    /// * `Err(error)` - Cache operation or computation failed
    /// 
    /// # Example
    /// ```rust
    /// // Simple cache get (existing behavior)
    /// let cached_data = cache_manager.get_with_fallback("my_key", None, None).await?;
    /// 
    /// // Get with computation fallback (new enhanced behavior)
    /// let market_data = cache_manager.get_with_fallback(
    ///     "latest_price", 
    ///     Some(|| async { fetch_price_from_api().await }),
    ///     Some(CacheStrategy::RealTime)
    /// ).await?;
    /// ```
    
    /// Set value with specific cache strategy (both L1 and L2)
    pub async fn set_with_strategy(&self, key: &str, value: serde_json::Value, strategy: CacheStrategy) -> Result<()> {
        let ttl = strategy.to_duration();
        
        // Store in both L1 and L2
        let l1_result = self.l1_cache.set_with_ttl(key, value.clone(), ttl).await;
        let l2_result = self.l2_cache.set_with_ttl(key, value, ttl).await;
        
        // Return success if at least one cache succeeded
        match (l1_result, l2_result) {
            (Ok(_), Ok(_)) => {
                // Both succeeded
                println!("💾 [L1+L2] Cached '{}' with TTL {:?}", key, ttl);
                Ok(())
            }
            (Ok(_), Err(_)) => {
                // L1 succeeded, L2 failed
                eprintln!("⚠️ L2 cache set failed for key '{}', continuing with L1", key);
                println!("💾 [L1] Cached '{}' with TTL {:?}", key, ttl);
                Ok(())
            }
            (Err(_), Ok(_)) => {
                // L1 failed, L2 succeeded
                eprintln!("⚠️ L1 cache set failed for key '{}', continuing with L2", key);
                println!("💾 [L2] Cached '{}' with TTL {:?}", key, ttl);
                Ok(())
            }
            (Err(e1), Err(_e2)) => {
                // Both failed
                Err(anyhow::anyhow!("Both L1 and L2 cache set failed for key '{}': {}", key, e1))
            }
        }
    }
    
    /// Get or compute value with Cache Stampede protection across L1+L2+Compute
    /// 
    /// This method provides comprehensive Cache Stampede protection:
    /// 1. Check L1 cache first (uses Moka's built-in coalescing)
    /// 2. Check L2 cache with mutex-based coalescing
    /// 3. Compute fresh data with protection against concurrent computations
    /// 
    /// # Arguments
    /// * `key` - Cache key
    /// * `strategy` - Cache strategy for TTL and storage behavior
    /// * `compute_fn` - Async function to compute the value if not in any cache
    /// 
    /// # Example
    /// ```rust
    /// let market_data = cache_manager.get_or_compute_with(
    ///     "latest_btc_price",
    ///     CacheStrategy::RealTime,
    ///     || async {
    ///         fetch_btc_price_from_api().await
    ///     }
    /// ).await?;
    /// ```
    #[allow(dead_code)]
    pub async fn get_or_compute_with<F, Fut>(
        &self,
        key: &str,
        strategy: CacheStrategy,
        compute_fn: F,
    ) -> Result<serde_json::Value>
    where
        F: FnOnce() -> Fut + Send,
        Fut: Future<Output = Result<serde_json::Value>> + Send,
    {
        self.total_requests.fetch_add(1, Ordering::Relaxed);
        
        // 1. Try L1 cache first (with built-in Moka coalescing for hot data)
        if let Some(value) = self.l1_cache.get(key).await {
            self.l1_hits.fetch_add(1, Ordering::Relaxed);
            return Ok(value);
        }
        
        // 2. L1 miss - try L2 with Cache Stampede protection
        let key_owned = key.to_string();
        let lock_guard = self.in_flight_requests
            .entry(key_owned.clone())
            .or_insert_with(|| Arc::new(Mutex::new(())))
            .clone();
        
        let _guard = lock_guard.lock().await;
        
        // RAII cleanup guard - ensures entry is removed even on early return or panic
        let _cleanup_guard = CleanupGuard {
            map: &self.in_flight_requests,
            key: key_owned,
        };
        
        // 3. Double-check L1 cache after acquiring lock
        // (Another request might have populated it while we were waiting)
        if let Some(value) = self.l1_cache.get(key).await {
            self.l1_hits.fetch_add(1, Ordering::Relaxed);
            // _cleanup_guard will auto-remove entry on drop
            return Ok(value);
        }
        
        // 4. Check L2 cache
        if let Some(value) = self.l2_cache.get(key).await {
            self.l2_hits.fetch_add(1, Ordering::Relaxed);
            
            // Promote to L1 for future requests
            let ttl = strategy.to_duration();
            if let Err(e) = self.l1_cache.set_with_ttl(key, value.clone(), ttl).await {
                eprintln!("⚠️ Failed to promote key '{}' to L1: {}", key, e);
            } else {
                self.promotions.fetch_add(1, Ordering::Relaxed);
                println!("⬆️ Promoted '{}' from L2 to L1", key);
            }
            
            // _cleanup_guard will auto-remove entry on drop
            return Ok(value);
        }
        
        // 5. Both L1 and L2 miss - compute fresh data
        println!("💻 Computing fresh data for key: '{}' (Cache Stampede protected)", key);
        let fresh_data = compute_fn().await?;
        
        // 6. Store in both caches
        if let Err(e) = self.set_with_strategy(key, fresh_data.clone(), strategy).await {
            eprintln!("⚠️ Failed to cache computed data for key '{}': {}", key, e);
        }
        
        // 7. _cleanup_guard will auto-remove entry on drop
        
        Ok(fresh_data)
    }
    
    /// Get comprehensive cache statistics
    #[allow(dead_code)]
    pub fn get_stats(&self) -> CacheManagerStats {
        let total_reqs = self.total_requests.load(Ordering::Relaxed);
        let l1_hits = self.l1_hits.load(Ordering::Relaxed);
        let l2_hits = self.l2_hits.load(Ordering::Relaxed);
        let misses = self.misses.load(Ordering::Relaxed);
        
        CacheManagerStats {
            total_requests: total_reqs,
            l1_hits,
            l2_hits,
            total_hits: l1_hits + l2_hits,
            misses,
            hit_rate: if total_reqs > 0 { 
                ((l1_hits + l2_hits) as f64 / total_reqs as f64) * 100.0 
            } else { 0.0 },
            l1_hit_rate: if total_reqs > 0 { 
                (l1_hits as f64 / total_reqs as f64) * 100.0 
            } else { 0.0 },
            promotions: self.promotions.load(Ordering::Relaxed),
            in_flight_requests: self.in_flight_requests.len(),
        }
    }
    
    // ===== Redis Streams Methods =====
    
    /// Publish data to Redis Stream
    /// 
    /// # Arguments
    /// * `stream_key` - Name of the stream (e.g., "market_data_stream")
    /// * `fields` - Field-value pairs to publish
    /// * `maxlen` - Optional max length for stream trimming
    /// 
    /// # Returns
    /// The entry ID generated by Redis
    pub async fn publish_to_stream(
        &self,
        stream_key: &str,
        fields: Vec<(String, String)>,
        maxlen: Option<usize>
    ) -> Result<String> {
        self.l2_cache.stream_add(stream_key, fields, maxlen).await
    }
    
    /// Read latest entries from Redis Stream
    /// 
    /// # Arguments
    /// * `stream_key` - Name of the stream
    /// * `count` - Number of latest entries to retrieve
    /// 
    /// # Returns
    /// Vector of (entry_id, fields) tuples (newest first)
    pub async fn read_stream_latest(
        &self,
        stream_key: &str,
        count: usize
    ) -> Result<Vec<(String, Vec<(String, String)>)>> {
        self.l2_cache.stream_read_latest(stream_key, count).await
    }
    
    /// Read from Redis Stream with optional blocking
    /// 
    /// # Arguments
    /// * `stream_key` - Name of the stream
    /// * `last_id` - Last ID seen ("0" for start, "$" for new only)
    /// * `count` - Max entries to retrieve
    /// * `block_ms` - Optional blocking timeout in ms
    /// 
    /// # Returns
    /// Vector of (entry_id, fields) tuples
    pub async fn read_stream(
        &self,
        stream_key: &str,
        last_id: &str,
        count: usize,
        block_ms: Option<usize>
    ) -> Result<Vec<(String, Vec<(String, String)>)>> {
        self.l2_cache.stream_read(stream_key, last_id, count, block_ms).await
    }
}

/// Cache Manager statistics
#[allow(dead_code)]
#[derive(Debug, Clone)]
pub struct CacheManagerStats {
    pub total_requests: u64,
    pub l1_hits: u64,
    pub l2_hits: u64,
    pub total_hits: u64,
    pub misses: u64,
    pub hit_rate: f64,
    pub l1_hit_rate: f64,
    pub promotions: usize,
    pub in_flight_requests: usize,
}
